{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q tqdm\n",
    "!pip install -q tensorboard\n",
    "!pip install -Uq segmentation-models-pytorch\n",
    "!pip install -q pytorch-lightning\n",
    "!pip install -q monai\n",
    "!pip install -q timm\n",
    "!pip install -q transformers\n",
    "!pip install -q torchmetrics\n",
    "!pip install -q kornia\n",
    "!pip install -q opencv-python\n",
    "# !pip install -Uq openmim\n",
    "# !mim install -q mmcv-full\n",
    "# !pip install -q mmsegmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import random\n",
    "\n",
    "import kornia as K\n",
    "import kornia.augmentation as KA\n",
    "import numpy as np\n",
    "import cv2\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.encoders import get_preprocessing_fn\n",
    "import torch\n",
    "import torchmetrics\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchmetrics\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import draw_segmentation_masks\n",
    "from tqdm import tqdm\n",
    "from transformers import ViTModel\n",
    "import pdb\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, ModelSummary\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-14T08:27:52.469056Z",
     "iopub.status.busy": "2023-02-14T08:27:52.468699Z",
     "iopub.status.idle": "2023-02-14T08:27:52.476964Z",
     "shell.execute_reply": "2023-02-14T08:27:52.475987Z",
     "shell.execute_reply.started": "2023-02-14T08:27:52.469026Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_subject_image_idx(path: str) -> tuple:\n",
    "    filename = os.path.splitext(os.path.basename(path))[0]\n",
    "    subject, image_idx = map(int, filename.split('_')[:2])\n",
    "    return subject, image_idx\n",
    "\n",
    "\n",
    "def get_data_path():\n",
    "    train_dir = '../input/ultrasound-nerve-segmentation/train'\n",
    "    data_paths = []\n",
    "\n",
    "    for filename in os.listdir(train_dir):\n",
    "        if filename.endswith(\"_mask.tif\"):\n",
    "            continue\n",
    "        elif filename.endswith(\".tif\"):\n",
    "            mask_file_name = os.path.splitext(filename)[0] + '_mask.tif'\n",
    "            input_img_path = os.path.join(train_dir, filename)\n",
    "            mask_path = os.path.join(train_dir, mask_file_name)\n",
    "            data_paths.append((input_img_path, mask_path))\n",
    "\n",
    "    return data_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-14T08:34:59.600765Z",
     "iopub.status.busy": "2023-02-14T08:34:59.599904Z",
     "iopub.status.idle": "2023-02-14T08:34:59.613171Z",
     "shell.execute_reply": "2023-02-14T08:34:59.612235Z",
     "shell.execute_reply.started": "2023-02-14T08:34:59.600731Z"
    }
   },
   "outputs": [],
   "source": [
    "class UltrasoundNerveDataset(Dataset):\n",
    "    \"\"\"Ultrasound image and Nerve structure dataset return the PIL image.\"\"\"\n",
    "\n",
    "    def __init__(self, is_val: bool = None, val_stride: int = 0, transform=None, preprocess=None):\n",
    "\n",
    "        self.data_paths = get_data_path()\n",
    "        random.Random(111).shuffle(self.data_paths)\n",
    "\n",
    "        if is_val:\n",
    "            assert val_stride > 0, val_stride\n",
    "            self.data_paths = self.data_paths[::val_stride]\n",
    "            assert self.data_paths\n",
    "        elif val_stride > 0:\n",
    "            del self.data_paths[::val_stride]\n",
    "            assert self.data_paths\n",
    "\n",
    "        print(\"{} {} samples\".format(len(self.data_paths), \"validation\" if is_val else \"training\"))\n",
    "\n",
    "        self.transform = transform\n",
    "        self.preprocess = preprocess\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        input_img_path, mask_path = self.data_paths[idx]\n",
    "        img = cv2.imread(input_img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        if self.transform:\n",
    "            img, mask = self.transform(img, mask)\n",
    "            \n",
    "        if self.preprocess:\n",
    "            sample = self.preprocess(image=img, mask=mask)\n",
    "            img, mask = sample['image'], sample['mask']\n",
    "\n",
    "        try:\n",
    "            assert img.shape == mask.shape\n",
    "        except AssertionError:\n",
    "            print(\"img.shape: {}, mask.shape: {}\".format(img.shape, mask.shape))\n",
    "            raise\n",
    "        \n",
    "\n",
    "        try:\n",
    "            assert mask.max().item() <= 1.\n",
    "        except AssertionError:\n",
    "            print(\"mask.max().item(): {}\".format(mask.max().item()))\n",
    "            raise\n",
    "            \n",
    "        sample = {'img': img, 'mask': mask}\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Lightning Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UltrasoundDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size: int = 16, val_stride: int = 5, num_workers: int = None, transform=None, preprocess=None):\n",
    "        super().__init__()\n",
    "#         self.save_hyperparameters()\n",
    "        self.batch_size = batch_size\n",
    "        self.val_stride = val_stride\n",
    "        self.transform = transform\n",
    "        self.preprocess = preprocess\n",
    "\n",
    "        if num_workers is None:\n",
    "            self.num_workers = os.cpu_count()\n",
    "        else:\n",
    "            self.num_workers = num_workers\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        UltrasoundNerveDataset(is_val=False, val_stride=self.val_stride, transform=self.transform, preprocess=self.preprocess)\n",
    "        UltrasoundNerveDataset(is_val=True, val_stride=self.val_stride, transform=self.transform, preprocess=self.preprocess)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = UltrasoundNerveDataset(is_val=False, val_stride=self.val_stride, transform=self.transform, preprocess=self.preprocess)\n",
    "        self.val_dataset = UltrasoundNerveDataset(is_val=True, val_stride=self.val_stride, transform=self.transform, preprocess=self.preprocess)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageToTensor(nn.Module):\n",
    "    \"\"\"Transform module which convert image to tensor, normalize, and resize to 224, 224.\"\"\"\n",
    "    def __init__(self, imsize: tuple=(224, 224)):\n",
    "        super().__init__()\n",
    "        self.resize = KA.Resize(imsize, keepdim=True, p=1, resample='nearest')\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def forward(self, x, y) -> torch.Tensor:\n",
    "\n",
    "        x_out = K.image_to_tensor(x, keepdim=True)\n",
    "        x_out = x_out.float()\n",
    "        x_out = torch.div(x_out, 255.)\n",
    "#         x_out = self.normalize(x_out)\n",
    "        x_out = self.resize(x_out)\n",
    "        x_out = torch.squeeze(x_out, 1)\n",
    "        \n",
    "        y_out = K.image_to_tensor(y, keepdim=True)\n",
    "        y_out = y_out.float()\n",
    "        y_out = torch.div(y_out, 255, rounding_mode='trunc')\n",
    "        y_out = self.resize(y_out)\n",
    "        \n",
    "        return x_out, y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Augmentation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.normalize = KA.Normalize(mean=(0.3890,), std=(0.2223,), p=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.normalize(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation Models Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-14T08:35:24.246582Z",
     "iopub.status.busy": "2023-02-14T08:35:24.246014Z",
     "iopub.status.idle": "2023-02-14T08:35:24.266010Z",
     "shell.execute_reply": "2023-02-14T08:35:24.265030Z",
     "shell.execute_reply.started": "2023-02-14T08:35:24.246538Z"
    }
   },
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, encoder_name='resnet34', encoder_weights='imagenet', classes=1, activation='sigmoid', augmentation=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = smp.Unet(encoder_name=encoder_name, encoder_weights=encoder_weights, in_channels=1, classes=classes, activation=activation)\n",
    "        \n",
    "        self.augmentation = augmentation\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.augmentation:\n",
    "            x = self.augmentation(x)\n",
    "        out = self.encoder(x)\n",
    "        return out\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MONAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def vit():\n",
    "#     model = nets.ViT(in_channels=1, img_size=420*580, patch_size=8)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def swin():\n",
    "#     return timm.create_model('swin_base_patch4_window7_448')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルを選択する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-14T08:35:27.214779Z",
     "iopub.status.busy": "2023-02-14T08:35:27.213947Z",
     "iopub.status.idle": "2023-02-14T08:35:54.709427Z",
     "shell.execute_reply": "2023-02-14T08:35:54.708244Z",
     "shell.execute_reply.started": "2023-02-14T08:35:27.214739Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def select_model(model_str: str):\n",
    "    model = None\n",
    "    model_str = model_str.lower()\n",
    "    if model_str == \"debug\":\n",
    "        model = ModelForDebug()\n",
    "    elif model_str == 'unet':\n",
    "        model = UNet(augmentation=Augmentation())\n",
    "    elif model_str == 'unet_bn':\n",
    "        model = UNetBatchNorm(imsize=420*580)\n",
    "    elif model_str == 'unet_plus_plus':\n",
    "        model = UNetPlusPlus(imsize=420*580)\n",
    "    elif model_str == 'manet':\n",
    "        model = MAnet(imsize=420*580)\n",
    "    elif model_str == 'pspnet':\n",
    "        model = PSPNet(imsize=420*580)\n",
    "    elif model_str == 'linknet':\n",
    "        model = LinkNet(imsize=420*580)\n",
    "    elif model_str == 'deep_lab_v3_plus':\n",
    "        model = DeepLabV3Plus(imsize=420*580)\n",
    "    # TODO ここから下は未実装\n",
    "    # elif model_str == 'vit':\n",
    "    #     model = Vit()\n",
    "    # elif model_str == 'swin':\n",
    "    #     model = swin()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 損失関数と指標定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-14T08:35:18.101572Z",
     "iopub.status.busy": "2023-02-14T08:35:18.101097Z",
     "iopub.status.idle": "2023-02-14T08:35:18.121334Z",
     "shell.execute_reply": "2023-02-14T08:35:18.120399Z",
     "shell.execute_reply.started": "2023-02-14T08:35:18.101530Z"
    }
   },
   "outputs": [],
   "source": [
    "class DiceLoss():\n",
    "    def __call__(self, prediction, target):\n",
    "        loss = self.calculate_loss(prediction, target)\n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_loss(prediction, target):\n",
    "        epsilon = 1\n",
    "\n",
    "        dice_prediction = prediction.sum(dim=[1, 2, 3])    # 要素iには、バッチiの、各ピクセルについて陽性である確率の総和\n",
    "        dice_label = target.sum(dim=[1, 2, 3])    # 要素iには、バッチiの陽性ラベルの数\n",
    "        dice_correct = (target * prediction).sum(dim=[1, 2, 3])\n",
    "        \n",
    "        dice_ratio = (2 * dice_correct + epsilon) / (dice_prediction + dice_label + epsilon)\n",
    "\n",
    "        return 1 - dice_ratio\n",
    "    \n",
    "    \n",
    "class SegmentationMetrics():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.f1_cal = torchmetrics.F1Score(task='binary')\n",
    "        self.dice_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-14T08:35:18.101572Z",
     "iopub.status.busy": "2023-02-14T08:35:18.101097Z",
     "iopub.status.idle": "2023-02-14T08:35:18.121334Z",
     "shell.execute_reply": "2023-02-14T08:35:18.120399Z",
     "shell.execute_reply.started": "2023-02-14T08:35:18.101530Z"
    }
   },
   "outputs": [],
   "source": [
    "class DiceLoss():\n",
    "    def __call__(self, prediction, target):\n",
    "        loss = self.calculate_loss(prediction, target)\n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_loss(prediction, target):\n",
    "        epsilon = 1\n",
    "\n",
    "        dice_prediction = prediction.sum(dim=[1, 2, 3])    # 要素iには、バッチiの、各ピクセルについて陽性である確率の総和\n",
    "        dice_label = target.sum(dim=[1, 2, 3])    # 要素iには、バッチiの陽性ラベルの数\n",
    "        dice_correct = (target * prediction).sum(dim=[1, 2, 3])\n",
    "        \n",
    "        dice_ratio = (2 * dice_correct + epsilon) / (dice_prediction + dice_label + epsilon)\n",
    "\n",
    "        return 1 - dice_ratio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Lightning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitSegmentationModule(pl.LightningModule):\n",
    "    def __init__(self, model, lr=0.05):\n",
    "        super().__init__()\n",
    "        self.model = select_model(model_str=model)\n",
    "        self.lr = lr\n",
    "        self.save_hyperparameters()\n",
    "        self.example_input_array = torch.Tensor(8, 1, 224, 224)\n",
    "        self.loss_func = self.configure_loss_function()\n",
    "        self.training_step_outputs = {}\n",
    "        self.validation_step_outputs = {}\n",
    "        \n",
    "        self.f1_cal = torchmetrics.F1Score(task='binary')\n",
    "        self.recall_cal = torchmetrics.Recall(task='binary')\n",
    "        self.precision_cal = torchmetrics.Precision(task='binary')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        img, target = batch['img'], batch['mask']\n",
    "        target = target.to(torch.uint8)\n",
    "        prediction = self.model(img)\n",
    "        print(f\"Prediction Max: {prediction.max().item()}, Min: {prediction.min().item()}, Mean: {prediction.mean()}, Std: {prediction.std()}\")\n",
    "        loss = self.loss_func(prediction, target)\n",
    "#         loss_mean = loss.mean()\n",
    "#         self.log('train_loss', loss_mean, on_epoch=True)\n",
    "        self.log('train_loss', loss, on_epoch=True)\n",
    "        output_dict = self.draw_mask_on_image(batch, prediction, target)\n",
    "        output_dict['loss'] = loss\n",
    "        self.training_step_outputs = output_dict\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        img, target = batch['img'], batch['mask']\n",
    "        target = target.view(img.shape)\n",
    "        prediction = self.model(img)\n",
    "        loss = self.loss_func(prediction, target)\n",
    "#         loss_mean = loss.mean()\n",
    "        metrics = self.calculate_metrics(prediction, target)\n",
    "#         metrics['val_loss'] = loss_mean\n",
    "        metrics['val_loss'] = loss\n",
    "        self.log_dict(metrics, on_epoch=True)\n",
    "        output_dict = self.draw_mask_on_image(batch, prediction, target)\n",
    "        output_dict['loss'] = loss\n",
    "        self.validation_step_outputs = output_dict\n",
    "#         return loss_mean\n",
    "        return loss\n",
    "\n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        if not self.training_step_outputs:\n",
    "            return\n",
    "        training_outputs = self.training_step_outputs\n",
    "        validation_outputs = self.validation_step_outputs\n",
    "        logger = self.logger.experiment\n",
    "        \n",
    "        epochs = self.current_epoch\n",
    "        \n",
    "        for mode_str, output in zip(['training', 'val'], [training_outputs, validation_outputs]):\n",
    "            self.add_image(mode_str, output, epochs, logger)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def calculate_metrics(self, prediction, target):\n",
    "        f1 = self.f1_cal(preds=prediction, target=target)\n",
    "        precision = self.precision_cal(preds=prediction, target=target)\n",
    "        recall = self.recall_cal(preds=prediction, target=target)\n",
    "        \n",
    "        metrics_dict = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }\n",
    "\n",
    "        return metrics_dict\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def draw_mask_on_image(self, batch, predictions, target):\n",
    "        self.model.eval()\n",
    "\n",
    "        # visualize image\n",
    "        imgs_vis = batch['img'].cpu()\n",
    "        imgs_vis *= 255\n",
    "        imgs_vis = imgs_vis.to(torch.uint8)\n",
    "        imgs_rgb = K.color.grayscale_to_rgb(imgs_vis)\n",
    "\n",
    "        # target mask\n",
    "        targets = torch.squeeze(target, 1)\n",
    "        targets = targets.to(torch.bool).cpu()\n",
    "\n",
    "        # output mask\n",
    "        predictions = predictions >= 0.5\n",
    "        predictions = torch.squeeze(predictions, 1).cpu()\n",
    "\n",
    "        # mask\n",
    "        output_with_mask = [draw_segmentation_masks(img_rgb, masks=output, alpha=.3, colors=\"#FFFFFF\") for img_rgb, output in zip(imgs_rgb, predictions)]\n",
    "        target_with_mask = [draw_segmentation_masks(img_rgb, masks=target, alpha=.3, colors=\"#FFFFFF\") for img_rgb, target in zip(imgs_rgb, targets)]\n",
    "        \n",
    "        output_dict = {'output': output_with_mask, 'target': target_with_mask}\n",
    "        return output_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def add_image(mode_str, output, epochs, logger):\n",
    "#         for image_ndx, (output_mask, target_mask, loss) in enumerate(zip(output['output'], output['target'], output['loss'])):\n",
    "        for image_ndx, (output_mask, target_mask) in enumerate(zip(output['output'], output['target'])):\n",
    "            loss = output['loss']\n",
    "            logger.add_image(f'{mode_str}/E{epochs}_#{image_ndx}_prediction_{loss.item():.3f}loss', output_mask, dataformats='CHW')\n",
    "            logger.add_image(f'{mode_str}/E{epochs}_#{image_ndx}_label', target_mask, dataformats='CHW')\n",
    "            logger.flush()\n",
    "\n",
    "    \n",
    "    def configure_loss_function(self):\n",
    "#         loss_func = DiceLoss()\n",
    "        loss_func = smp.losses.DiceLoss('binary', from_logits=False)\n",
    "        return loss_func\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3329)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = torch.ones(8, 1, 224, 224)\n",
    "target = torch.randint(2, (8, 1, 224, 224))\n",
    "loss = smp.losses.DiceLoss('binary', from_logits=False, smooth=1, eps=0)\n",
    "loss(preds, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /home/ec2-user/SageMaker/working/logs/20230312_02.38UNet_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                                         | Type             | Params | In sizes                                                                                                   | Out sizes                                                                                                 \n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "0  | model                                        | UNet             | 24.4 M | [8, 1, 224, 224]                                                                                           | [8, 1, 224, 224]                                                                                          \n",
      "1  | model.encoder                                | Unet             | 24.4 M | [8, 1, 224, 224]                                                                                           | [8, 1, 224, 224]                                                                                          \n",
      "2  | model.encoder.encoder                        | ResNetEncoder    | 21.3 M | [8, 1, 224, 224]                                                                                           | [[8, 1, 224, 224], [8, 64, 112, 112], [8, 64, 56, 56], [8, 128, 28, 28], [8, 256, 14, 14], [8, 512, 7, 7]]\n",
      "3  | model.encoder.encoder.conv1                  | Conv2d           | 3.1 K  | [8, 1, 224, 224]                                                                                           | [8, 64, 112, 112]                                                                                         \n",
      "4  | model.encoder.encoder.bn1                    | BatchNorm2d      | 128    | [8, 64, 112, 112]                                                                                          | [8, 64, 112, 112]                                                                                         \n",
      "5  | model.encoder.encoder.relu                   | ReLU             | 0      | [8, 64, 112, 112]                                                                                          | [8, 64, 112, 112]                                                                                         \n",
      "6  | model.encoder.encoder.maxpool                | MaxPool2d        | 0      | [8, 64, 112, 112]                                                                                          | [8, 64, 56, 56]                                                                                           \n",
      "7  | model.encoder.encoder.layer1                 | Sequential       | 221 K  | [8, 64, 56, 56]                                                                                            | [8, 64, 56, 56]                                                                                           \n",
      "8  | model.encoder.encoder.layer1.0               | BasicBlock       | 74.0 K | [8, 64, 56, 56]                                                                                            | [8, 64, 56, 56]                                                                                           \n",
      "9  | model.encoder.encoder.layer1.1               | BasicBlock       | 74.0 K | [8, 64, 56, 56]                                                                                            | [8, 64, 56, 56]                                                                                           \n",
      "10 | model.encoder.encoder.layer1.2               | BasicBlock       | 74.0 K | [8, 64, 56, 56]                                                                                            | [8, 64, 56, 56]                                                                                           \n",
      "11 | model.encoder.encoder.layer2                 | Sequential       | 1.1 M  | [8, 64, 56, 56]                                                                                            | [8, 128, 28, 28]                                                                                          \n",
      "12 | model.encoder.encoder.layer2.0               | BasicBlock       | 230 K  | [8, 64, 56, 56]                                                                                            | [8, 128, 28, 28]                                                                                          \n",
      "13 | model.encoder.encoder.layer2.1               | BasicBlock       | 295 K  | [8, 128, 28, 28]                                                                                           | [8, 128, 28, 28]                                                                                          \n",
      "14 | model.encoder.encoder.layer2.2               | BasicBlock       | 295 K  | [8, 128, 28, 28]                                                                                           | [8, 128, 28, 28]                                                                                          \n",
      "15 | model.encoder.encoder.layer2.3               | BasicBlock       | 295 K  | [8, 128, 28, 28]                                                                                           | [8, 128, 28, 28]                                                                                          \n",
      "16 | model.encoder.encoder.layer3                 | Sequential       | 6.8 M  | [8, 128, 28, 28]                                                                                           | [8, 256, 14, 14]                                                                                          \n",
      "17 | model.encoder.encoder.layer3.0               | BasicBlock       | 919 K  | [8, 128, 28, 28]                                                                                           | [8, 256, 14, 14]                                                                                          \n",
      "18 | model.encoder.encoder.layer3.1               | BasicBlock       | 1.2 M  | [8, 256, 14, 14]                                                                                           | [8, 256, 14, 14]                                                                                          \n",
      "19 | model.encoder.encoder.layer3.2               | BasicBlock       | 1.2 M  | [8, 256, 14, 14]                                                                                           | [8, 256, 14, 14]                                                                                          \n",
      "20 | model.encoder.encoder.layer3.3               | BasicBlock       | 1.2 M  | [8, 256, 14, 14]                                                                                           | [8, 256, 14, 14]                                                                                          \n",
      "21 | model.encoder.encoder.layer3.4               | BasicBlock       | 1.2 M  | [8, 256, 14, 14]                                                                                           | [8, 256, 14, 14]                                                                                          \n",
      "22 | model.encoder.encoder.layer3.5               | BasicBlock       | 1.2 M  | [8, 256, 14, 14]                                                                                           | [8, 256, 14, 14]                                                                                          \n",
      "23 | model.encoder.encoder.layer4                 | Sequential       | 13.1 M | [8, 256, 14, 14]                                                                                           | [8, 512, 7, 7]                                                                                            \n",
      "24 | model.encoder.encoder.layer4.0               | BasicBlock       | 3.7 M  | [8, 256, 14, 14]                                                                                           | [8, 512, 7, 7]                                                                                            \n",
      "25 | model.encoder.encoder.layer4.1               | BasicBlock       | 4.7 M  | [8, 512, 7, 7]                                                                                             | [8, 512, 7, 7]                                                                                            \n",
      "26 | model.encoder.encoder.layer4.2               | BasicBlock       | 4.7 M  | [8, 512, 7, 7]                                                                                             | [8, 512, 7, 7]                                                                                            \n",
      "27 | model.encoder.decoder                        | UnetDecoder      | 3.2 M  | [[8, 1, 224, 224], [8, 64, 112, 112], [8, 64, 56, 56], [8, 128, 28, 28], [8, 256, 14, 14], [8, 512, 7, 7]] | [8, 16, 224, 224]                                                                                         \n",
      "28 | model.encoder.decoder.center                 | Identity         | 0      | [8, 512, 7, 7]                                                                                             | [8, 512, 7, 7]                                                                                            \n",
      "29 | model.encoder.decoder.blocks                 | ModuleList       | 3.2 M  | ?                                                                                                          | ?                                                                                                         \n",
      "30 | model.encoder.decoder.blocks.0               | DecoderBlock     | 2.4 M  | [[8, 512, 7, 7], [8, 256, 14, 14]]                                                                         | [8, 256, 14, 14]                                                                                          \n",
      "31 | model.encoder.decoder.blocks.1               | DecoderBlock     | 590 K  | [[8, 256, 14, 14], [8, 128, 28, 28]]                                                                       | [8, 128, 28, 28]                                                                                          \n",
      "32 | model.encoder.decoder.blocks.2               | DecoderBlock     | 147 K  | [[8, 128, 28, 28], [8, 64, 56, 56]]                                                                        | [8, 64, 56, 56]                                                                                           \n",
      "33 | model.encoder.decoder.blocks.3               | DecoderBlock     | 46.2 K | [[8, 64, 56, 56], [8, 64, 112, 112]]                                                                       | [8, 32, 112, 112]                                                                                         \n",
      "34 | model.encoder.decoder.blocks.4               | DecoderBlock     | 7.0 K  | [[8, 32, 112, 112], '?']                                                                                   | [8, 16, 224, 224]                                                                                         \n",
      "35 | model.encoder.segmentation_head              | SegmentationHead | 145    | [8, 16, 224, 224]                                                                                          | [8, 1, 224, 224]                                                                                          \n",
      "36 | model.encoder.segmentation_head.0            | Conv2d           | 145    | [8, 16, 224, 224]                                                                                          | [8, 1, 224, 224]                                                                                          \n",
      "37 | model.encoder.segmentation_head.1            | Identity         | 0      | [8, 1, 224, 224]                                                                                           | [8, 1, 224, 224]                                                                                          \n",
      "38 | model.encoder.segmentation_head.2            | Activation       | 0      | [8, 1, 224, 224]                                                                                           | [8, 1, 224, 224]                                                                                          \n",
      "39 | model.encoder.segmentation_head.2.activation | Sigmoid          | 0      | [8, 1, 224, 224]                                                                                           | [8, 1, 224, 224]                                                                                          \n",
      "40 | model.augmentation                           | Augmentation     | 0      | [8, 1, 224, 224]                                                                                           | [8, 1, 224, 224]                                                                                          \n",
      "41 | model.augmentation.normalize                 | Normalize        | 0      | [8, 1, 224, 224]                                                                                           | [8, 1, 224, 224]                                                                                          \n",
      "42 | loss_func                                    | DiceLoss         | 0      | ?                                                                                                          | ?                                                                                                         \n",
      "43 | f1_cal                                       | BinaryF1Score    | 0      | ?                                                                                                          | ?                                                                                                         \n",
      "44 | recall_cal                                   | BinaryRecall     | 0      | ?                                                                                                          | ?                                                                                                         \n",
      "45 | precision_cal                                | BinaryPrecision  | 0      | ?                                                                                                          | ?                                                                                                         \n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "24.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "24.4 M    Total params\n",
      "97.720    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4695 training samples\n",
      "940 validation samples\n",
      "4695 training samples\n",
      "940 validation samples\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009850740432739258,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4410705eba444380ad5aa7d0d8258581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_22329/3734063520.py\", line 37, in __getitem__\n    sample = self.preprocess(image=img, mask=mask)\nTypeError: preprocess_input() missing 1 required positional argument: 'x'\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_22329/2585975622.py\u001B[0m in \u001B[0;36m<cell line: 36>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[0mtrainer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpl\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTrainer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfast_dev_run\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdevices\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'auto'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maccelerator\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'auto'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdefault_root_dir\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdefault_root_dir\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlogger\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtensorboard_logger\u001B[0m\u001B[0;34m,\u001B[0m  \u001B[0mcallbacks\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcallbacks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmax_epochs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m40\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcheck_val_every_n_epoch\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mauto_scale_batch_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mauto_lr_find\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbenchmark\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_sanity_val_steps\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0menable_progress_bar\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     35\u001B[0m \u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtune\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata_module\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 36\u001B[0;31m \u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata_module\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    606\u001B[0m         \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_maybe_unwrap_optimized\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    607\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstrategy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_lightning_module\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 608\u001B[0;31m         call._call_and_handle_interrupt(\n\u001B[0m\u001B[1;32m    609\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit_impl\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_dataloaders\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mval_dataloaders\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdatamodule\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mckpt_path\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    610\u001B[0m         )\n",
      "\u001B[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py\u001B[0m in \u001B[0;36m_call_and_handle_interrupt\u001B[0;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m     36\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstrategy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlauncher\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlaunch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrainer_fn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrainer\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtrainer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     37\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 38\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mtrainer_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     39\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     40\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0m_TunerExitException\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001B[0m in \u001B[0;36m_fit_impl\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    648\u001B[0m             \u001B[0mmodel_connected\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlightning_module\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    649\u001B[0m         )\n\u001B[0;32m--> 650\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_run\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mckpt_path\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mckpt_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    651\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    652\u001B[0m         \u001B[0;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstopped\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001B[0m in \u001B[0;36m_run\u001B[0;34m(self, model, ckpt_path)\u001B[0m\n\u001B[1;32m   1110\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_checkpoint_connector\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mresume_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1111\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1112\u001B[0;31m         \u001B[0mresults\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_run_stage\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1113\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1114\u001B[0m         \u001B[0mlog\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdetail\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"{self.__class__.__name__}: trainer tearing down\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001B[0m in \u001B[0;36m_run_stage\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1189\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredicting\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1190\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_run_predict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1191\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_run_train\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1192\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1193\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_pre_training_routine\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001B[0m in \u001B[0;36m_run_train\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1212\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1213\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset_detect_anomaly\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_detect_anomaly\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1214\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit_loop\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1215\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1216\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_run_evaluate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0m_EVALUATE_OUTPUT\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/pytorch_lightning/loops/loop.py\u001B[0m in \u001B[0;36mrun\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    197\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    198\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_advance_start\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 199\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madvance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    200\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_advance_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    201\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_restarting\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py\u001B[0m in \u001B[0;36madvance\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    265\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_data_fetcher\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msetup\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataloader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_to_device\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mbatch_to_device\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    266\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprofiler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprofile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"run_training_epoch\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 267\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_outputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mepoch_loop\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_data_fetcher\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    268\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    269\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mon_advance_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/pytorch_lightning/loops/loop.py\u001B[0m in \u001B[0;36mrun\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    197\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    198\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_advance_start\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 199\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madvance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    200\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_advance_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    201\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_restarting\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\u001B[0m in \u001B[0;36madvance\u001B[0;34m(self, data_fetcher)\u001B[0m\n\u001B[1;32m    185\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata_fetcher\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mDataLoaderIterDataFetcher\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    186\u001B[0m             \u001B[0mbatch_idx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbatch_idx\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 187\u001B[0;31m             \u001B[0mbatch\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata_fetcher\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    188\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    189\u001B[0m             \u001B[0mbatch_idx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata_fetcher\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py\u001B[0m in \u001B[0;36m__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    182\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    183\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__next__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 184\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfetching_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    185\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    186\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mreset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py\u001B[0m in \u001B[0;36mfetching_function\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    263\u001B[0m             \u001B[0;31m# this will run only when no pre-fetching was done.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    264\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 265\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fetch_next_batch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataloader_iter\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    266\u001B[0m                 \u001B[0;31m# consume the batch we just fetched\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    267\u001B[0m                 \u001B[0mbatch\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbatches\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py\u001B[0m in \u001B[0;36m_fetch_next_batch\u001B[0;34m(self, iterator)\u001B[0m\n\u001B[1;32m    278\u001B[0m         \u001B[0mstart_output\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_fetch_start\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    279\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 280\u001B[0;31m             \u001B[0mbatch\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    281\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mStopIteration\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    282\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_stop_profiler\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/pytorch_lightning/trainer/supporters.py\u001B[0m in \u001B[0;36m__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    567\u001B[0m             \u001B[0ma\u001B[0m \u001B[0mcollections\u001B[0m \u001B[0mof\u001B[0m \u001B[0mbatch\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    568\u001B[0m         \"\"\"\n\u001B[0;32m--> 569\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrequest_next_batch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloader_iters\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    570\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    571\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mstaticmethod\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/pytorch_lightning/trainer/supporters.py\u001B[0m in \u001B[0;36mrequest_next_batch\u001B[0;34m(loader_iters)\u001B[0m\n\u001B[1;32m    579\u001B[0m             \u001B[0mAny\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0ma\u001B[0m \u001B[0mcollections\u001B[0m \u001B[0mof\u001B[0m \u001B[0mbatch\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    580\u001B[0m         \"\"\"\n\u001B[0;32m--> 581\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mapply_to_collection\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mloader_iters\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mIterator\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    582\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    583\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mstaticmethod\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/lightning_utilities/core/apply_func.py\u001B[0m in \u001B[0;36mapply_to_collection\u001B[0;34m(data, dtype, function, wrong_dtype, include_none, allow_frozen, *args, **kwargs)\u001B[0m\n\u001B[1;32m     49\u001B[0m     \u001B[0;31m# Breaking condition\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     50\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mwrong_dtype\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mwrong_dtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 51\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mfunction\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     52\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     53\u001B[0m     \u001B[0melem_type\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001B[0m in \u001B[0;36m__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    626\u001B[0m                 \u001B[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    627\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_reset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[call-arg]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 628\u001B[0;31m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_next_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    629\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_num_yielded\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    630\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_dataset_kind\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0m_DatasetKind\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mIterable\u001B[0m \u001B[0;32mand\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001B[0m in \u001B[0;36m_next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1331\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1332\u001B[0m                 \u001B[0;32mdel\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_task_info\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1333\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_process_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1334\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1335\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_try_put_index\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001B[0m in \u001B[0;36m_process_data\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m   1357\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_try_put_index\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1358\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mExceptionWrapper\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1359\u001B[0;31m             \u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreraise\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1360\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1361\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/torch/_utils.py\u001B[0m in \u001B[0;36mreraise\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    541\u001B[0m             \u001B[0;31m# instantiate since we don't know how to\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    542\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mRuntimeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmsg\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 543\u001B[0;31m         \u001B[0;32mraise\u001B[0m \u001B[0mexception\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    544\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    545\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_22329/3734063520.py\", line 37, in __getitem__\n    sample = self.preprocess(image=img, mask=mask)\nTypeError: preprocess_input() missing 1 required positional argument: 'x'\n"
     ]
    }
   ],
   "source": [
    "# root_dir\n",
    "default_root_dir = os.path.join(os.path.dirname(os.path.abspath(os.getcwd())), 'working')\n",
    "log_dir = os.path.join(default_root_dir, 'logs')\n",
    "\n",
    "# Reproducibility\n",
    "seed_everything = pl.seed_everything(42, workers=True)\n",
    "\n",
    "# Dataloader\n",
    "preprocessing_fn = get_preprocessing_fn(\"resnet34\", pretrained=\"imagenet\")\n",
    "# Default: batch_size: int = 16, val_stride: int = 5, num_workers: int = None, transform=None, preprocess=None\n",
    "data_module = UltrasoundDataModule(batch_size=32, val_stride=6, num_workers=5, transform=ImageToTensor(), preprocess=preprocessing_fn)\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', verbose=True, patience=3)\n",
    "model_checkpoint = ModelCheckpoint(monitor='val_loss', verbose=True)\n",
    "model_summary = ModelSummary(max_depth=5)\n",
    "\n",
    "callbacks = [early_stopping, model_checkpoint, model_summary]\n",
    "\n",
    "# Model\n",
    "# model = \"debug\", \"Init\", \"UNet\", \"UNet_Pad\", \"UNet_BN\", \"UNet_with_library\", \"MANet\", \"Vit\", \"Swin\"\n",
    "model = LitSegmentationModule(model=\"UNet\")\n",
    "\n",
    "# Logger\n",
    "model_name = type(model.model).__name__\n",
    "JST = datetime.timezone(datetime.timedelta(hours=+9), 'JST')\n",
    "now_str = datetime.datetime.now(JST).strftime('%Y%m%d_%H.%M')\n",
    "log_name = now_str + model_name + '_logs'\n",
    "tensorboard_logger = TensorBoardLogger(save_dir=log_dir, name=log_name)\n",
    "\n",
    "# Training\n",
    "\n",
    "# TODO When submit the final results, change benchmark=False, and deterministic=True to ensure reproducibility.\n",
    "trainer = pl.Trainer(fast_dev_run=False, devices='auto', accelerator='auto', default_root_dir=default_root_dir, logger=tensorboard_logger,  callbacks=callbacks, max_epochs=40, check_val_every_n_epoch=3, auto_scale_batch_size=False, auto_lr_find=False, benchmark=True, num_sanity_val_steps=0, enable_progress_bar=True)\n",
    "trainer.tune(model, data_module)\n",
    "trainer.fit(model, data_module)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

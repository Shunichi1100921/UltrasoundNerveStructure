{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q tqdm\n",
    "!pip install -q tensorboard\n",
    "!pip install -Uq segmentation-models-pytorch\n",
    "!pip install -q pytorch-lightning\n",
    "!pip install -q monai\n",
    "!pip install -q timm\n",
    "!pip install -q transformers\n",
    "!pip install -q torchmetrics\n",
    "# !pip install -Uq openmim\n",
    "# !mim install -q mmcv-full\n",
    "# !pip install -q mmsegmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-06 23:11:13,579 - Created a temporary directory at /tmp/tmp12lgxvmp\n",
      "2023-03-06 23:11:13,580 - Writing /tmp/tmp12lgxvmp/_remote_module_non_scriptable.py\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import collections\n",
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "\n",
    "from monai.networks import nets\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "import torchmetrics\n",
    "import torchvision.transforms.functional as F  # TODO Fにするのは, transforms.functionalなのか、nn.functionalなのか、両方しないのか\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import timm\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchmetrics\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import draw_segmentation_masks\n",
    "from tqdm import tqdm\n",
    "from transformers import ViTModel\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import Callback, EarlyStopping, ModelCheckpoint, ModelSummary\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-14T08:27:52.469056Z",
     "iopub.status.busy": "2023-02-14T08:27:52.468699Z",
     "iopub.status.idle": "2023-02-14T08:27:52.476964Z",
     "shell.execute_reply": "2023-02-14T08:27:52.475987Z",
     "shell.execute_reply.started": "2023-02-14T08:27:52.469026Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_subject_image_idx(path: str) -> tuple:\n",
    "    filename = os.path.splitext(os.path.basename(path))[0]\n",
    "    subject, image_idx = map(int, filename.split('_')[:2])\n",
    "    return subject, image_idx\n",
    "\n",
    "\n",
    "def get_data_path():\n",
    "    train_dir = '../input/ultrasound-nerve-segmentation/train'\n",
    "    input_img_paths = []\n",
    "    target_paths = []\n",
    "\n",
    "    for filename in os.listdir(train_dir):\n",
    "        if filename.endswith(\"mask.tif\"):\n",
    "            target_paths.append(os.path.join(train_dir, filename))\n",
    "        elif filename.endswith(\".tif\"):\n",
    "            input_img_paths.append(os.path.join(train_dir, filename))\n",
    "\n",
    "    input_img_paths.sort(key=lambda x: get_subject_image_idx(x))\n",
    "    target_paths.sort(key=lambda x: get_subject_image_idx(x))\n",
    "    data_paths = [(input_img, target) for input_img, target in zip(input_img_paths, target_paths)]\n",
    "\n",
    "    return data_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-14T08:34:21.948335Z",
     "iopub.status.busy": "2023-02-14T08:34:21.947883Z",
     "iopub.status.idle": "2023-02-14T08:34:22.120142Z",
     "shell.execute_reply": "2023-02-14T08:34:22.119229Z",
     "shell.execute_reply.started": "2023-02-14T08:34:21.948296Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5635 samples in total.\n"
     ]
    }
   ],
   "source": [
    "def in_nerve(target_path):\n",
    "    target = Image.open(target_path)\n",
    "    return target.getextrema()[1] > 0\n",
    "\n",
    "num_data = len(get_data_path())\n",
    "print('{} samples in total.'.format(num_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasetの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-14T08:34:59.600765Z",
     "iopub.status.busy": "2023-02-14T08:34:59.599904Z",
     "iopub.status.idle": "2023-02-14T08:34:59.613171Z",
     "shell.execute_reply": "2023-02-14T08:34:59.612235Z",
     "shell.execute_reply.started": "2023-02-14T08:34:59.600731Z"
    }
   },
   "outputs": [],
   "source": [
    "class UltrasoundNerveDataset(Dataset):\n",
    "    \"\"\"Ultrasound image and Nerve structure dataset return the PIL image.\"\"\"\n",
    "\n",
    "    def __init__(self, is_val: bool=None, val_stride: int=0, only_nerve_imgs: bool=False, balance: bool=False, transform=None):\n",
    "        self.data_paths = get_data_path()\n",
    "        random.Random(111).shuffle(self.data_paths)\n",
    "\n",
    "        if only_nerve_imgs:\n",
    "            self.data_paths = [(input_img, target) for input_img, target in self.data_paths if in_nerve(target)]\n",
    "            assert self.data_paths\n",
    "            \n",
    "        if balance:\n",
    "            positive_data_paths = []\n",
    "            negative_data_paths = []\n",
    "            for input_img, target in self.data_paths:\n",
    "                if in_nerve(target):\n",
    "                    positive_data_paths.append((input_img, target))\n",
    "                else:\n",
    "                    negative_data_paths.append((input_img, target))\n",
    "            self.data_paths = positive_data_paths + negative_data_paths[:len(positive_data_paths)]\n",
    "            assert self.data_paths\n",
    "\n",
    "        if is_val:\n",
    "            assert val_stride > 0, val_stride\n",
    "            self.data_paths = self.data_paths[::val_stride]\n",
    "            assert self.data_paths\n",
    "        elif val_stride > 0:\n",
    "            del self.data_paths[::val_stride]\n",
    "            assert self.data_paths\n",
    "\n",
    "        print(\"{} {} samples\".format(len(self.data_paths), \"validation\" if is_val else \"training\"))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        input_img_path, target_path = self.data_paths[idx]\n",
    "        img = Image.open(input_img_path)\n",
    "        target = Image.open(target_path)\n",
    "\n",
    "        sample = {'img': img, 'target': target}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToTensor\n",
    "# - Image: Tensorにして、MaxMinScale\n",
    "# - Target: そのままTensorにする\n",
    "\n",
    "class PILToTensor:\n",
    "    def __call__(self, sample):\n",
    "        img_pil, target = sample['img'], sample['target']\n",
    "        img = F.to_tensor(img_pil)\n",
    "        img_vis = F.pil_to_tensor(img_pil)\n",
    "        target = torch.as_tensor(np.array(target), dtype=torch.float64)\n",
    "        target = torch.div(target, 255, rounding_mode='floor')\n",
    "        sample = {'img': img, 'target': target, 'img_vis': img_vis}\n",
    "        return sample\n",
    "\n",
    "class Normalize:\n",
    "    def __init__(self):\n",
    "        # Mean and std are calculated with all data.\n",
    "        self.mean = 0.3898\n",
    "        self.std = 0.2219\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        img = sample['img']\n",
    "        img = F.normalize(img, self.mean, self.std)\n",
    "        sample['img'] = img\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_transform():\n",
    "    return transforms.Compose([\n",
    "        PILToTensor(),\n",
    "        Normalize(),\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Lightning Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-14T08:35:18.904037Z",
     "iopub.status.busy": "2023-02-14T08:35:18.903614Z",
     "iopub.status.idle": "2023-02-14T08:35:18.914100Z",
     "shell.execute_reply": "2023-02-14T08:35:18.912998Z",
     "shell.execute_reply.started": "2023-02-14T08:35:18.903997Z"
    }
   },
   "outputs": [],
   "source": [
    "class UltrasoundDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, only_nerve_imgs: bool=False, balance: bool=False, transform=None, batch_size=8):\n",
    "        super().__init__()\n",
    "        self.transform = transform\n",
    "        if self.transform is None:\n",
    "            self.transform = transforms.Compose([PILToTensor(), Normalize()])\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def setup(self, stage):\n",
    "        self.train_dataset = UltrasoundNerveDataset(is_val=False, val_stride=3, only_nerve_imgs=self.hparams.only_nerve_imgs, balance=self.hparams.balance, transform=self.transform)\n",
    "        self.val_dataset = UltrasoundNerveDataset(is_val=True, val_stride=3, only_nerve_imgs=self.hparams.only_nerve_imgs, balance=self.hparams.balance, transform=self.transform)\n",
    "        if stage == 'test':\n",
    "            self.test_dataset = UltrasoundNerveDataset(transform=self.transform)\n",
    "        if stage == 'fit':\n",
    "            self.predict_dataset = UltrasoundNerveDataset(transform=self.transform)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.hparams.batch_size, shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.hparams.batch_size, shuffle=True)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.hparams.batch_size, shuffle=True)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.predict_dataset, batch_size=self.hparams.batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model for debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-14T08:35:25.548081Z",
     "iopub.status.busy": "2023-02-14T08:35:25.547066Z",
     "iopub.status.idle": "2023-02-14T08:35:25.554678Z",
     "shell.execute_reply": "2023-02-14T08:35:25.553561Z",
     "shell.execute_reply.started": "2023-02-14T08:35:25.548044Z"
    }
   },
   "outputs": [],
   "source": [
    "class ModelForDebug(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, 3, padding=\"same\", padding_mode=\"zeros\")\n",
    "        self.conv2 = nn.Conv2d(64, 1, 3, padding=\"same\", padding_mode=\"zeros\")\n",
    "        self.batch_norm = nn.BatchNorm2d(64)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.batch_norm(self.conv1(x))\n",
    "        out = torch.sigmoid(self.conv2(out))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNet with Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-14T08:35:23.562637Z",
     "iopub.status.busy": "2023-02-14T08:35:23.562271Z",
     "iopub.status.idle": "2023-02-14T08:35:23.582086Z",
     "shell.execute_reply": "2023-02-14T08:35:23.580865Z",
     "shell.execute_reply.started": "2023-02-14T08:35:23.562605Z"
    }
   },
   "outputs": [],
   "source": [
    "class UNetBatchNormConvBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, kernel_size=3, activation=torch.relu):\n",
    "        super(UNetBatchNormConvBlock, self).__init__()\n",
    "        self.batch_norm = nn.BatchNorm2d(out_size)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(out_size)\n",
    "        self.conv = nn.Conv2d(in_size, out_size, kernel_size)\n",
    "        self.conv2 = nn.Conv2d(out_size, out_size, kernel_size)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.activation(self.batch_norm(self.conv(x)))\n",
    "        out = self.activation(self.batch_norm2(self.conv2(out)))\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class UNetBatchNormUpBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, kernel_size=3, activation=torch.relu):\n",
    "        super(UNetBatchNormUpBlock, self).__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_size, out_size, 2, stride=2)\n",
    "        self.conv = nn.Conv2d(in_size, out_size, kernel_size)\n",
    "        self.conv2 = nn.Conv2d(out_size, out_size, kernel_size)\n",
    "        self.batch_norm = nn.BatchNorm2d(out_size)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(out_size)\n",
    "        self.activation = activation\n",
    "\n",
    "    def center_crop(self, layer: torch.Tensor, target_height, target_width):\n",
    "        batch_size, n_channels, layer_height, layer_width = layer.size()\n",
    "        height_remove = (layer_height - target_height) // 2\n",
    "        width_remove = (layer_width - target_width) // 2\n",
    "        return layer[:, :, height_remove:(height_remove + target_height), width_remove:(width_remove + target_width)]\n",
    "\n",
    "    def forward(self, x, bridge):\n",
    "        up = self.up(x)\n",
    "        crop1 = self.center_crop(bridge, up.size()[2], up.size()[3])\n",
    "        out = torch.cat([up, crop1], 1)\n",
    "        out = self.activation(self.batch_norm(self.conv(out)))\n",
    "        out = self.activation(self.batch_norm2(self.conv2(out)))\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class UNetBatchNorm(nn.Module):\n",
    "    def __init__(self, imsize):\n",
    "        super(UNetBatchNorm, self).__init__()\n",
    "        self.imsize = imsize\n",
    "\n",
    "        self.activation = torch.relu\n",
    "\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv_block1_64 = UNetBatchNormConvBlock(1, 64)\n",
    "        self.conv_block64_128 = UNetBatchNormConvBlock(64, 128)\n",
    "        self.conv_block128_256 = UNetBatchNormConvBlock(128, 256)\n",
    "        self.conv_block256_512 = UNetBatchNormConvBlock(256, 512)\n",
    "\n",
    "        self.up_block512_256 = UNetBatchNormUpBlock(512, 256)\n",
    "        self.up_block256_128 = UNetBatchNormUpBlock(256, 128)\n",
    "        self.up_block128_64 = UNetBatchNormUpBlock(128, 64)\n",
    "\n",
    "        self.last = nn.Conv2d(64, 1, 1)\n",
    "        self.pad = nn.ConstantPad2d(44, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        block1 = self.conv_block1_64(x)\n",
    "        pool1 = self.pool1(block1)\n",
    "\n",
    "        block2 = self.conv_block64_128(pool1)\n",
    "        pool2 = self.pool2(block2)\n",
    "\n",
    "        block3 = self.conv_block128_256(pool2)\n",
    "        pool3 = self.pool3(block3)\n",
    "\n",
    "        block4 = self.conv_block256_512(pool3)\n",
    "\n",
    "        up1 = self.up_block512_256(block4, block3)\n",
    "\n",
    "        up2 = self.up_block256_128(up1, block2)\n",
    "\n",
    "        up3 = self.up_block128_64(up2, block1)\n",
    "\n",
    "        return self.pad(torch.sigmoid(self.last(up3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation Models Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-14T08:35:24.246582Z",
     "iopub.status.busy": "2023-02-14T08:35:24.246014Z",
     "iopub.status.idle": "2023-02-14T08:35:24.266010Z",
     "shell.execute_reply": "2023-02-14T08:35:24.265030Z",
     "shell.execute_reply.started": "2023-02-14T08:35:24.246538Z"
    }
   },
   "outputs": [],
   "source": [
    "class UNetPlusPlus(nn.Module):\n",
    "    def __init__(self, imsize):\n",
    "        super(UNetPlusPlus, self).__init__()\n",
    "        self.imsize = imsize\n",
    "        self.pad = nn.ConstantPad2d(14, 0)\n",
    "        self.model = smp.UnetPlusPlus(\n",
    "            encoder_name=\"resnet34\",\n",
    "            encoder_weights=\"imagenet\",\n",
    "            in_channels=1,\n",
    "            classes=1,\n",
    "            activation='sigmoid'\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pad(x)\n",
    "        out =  self.model(x)\n",
    "        out = F.crop(out, 14, 14, 420, 580)\n",
    "        return out\n",
    "\n",
    "class PSPNet(nn.Module):\n",
    "    def __init__(self, imsize):\n",
    "        super(PSPNet, self).__init__()\n",
    "        self.imsize = imsize\n",
    "        self.pad = nn.ConstantPad2d(14, 0)\n",
    "        self.model = smp.PSPNet(\n",
    "            encoder_name=\"resnet34\",\n",
    "            encoder_weights=\"imagenet\",\n",
    "            in_channels=1,\n",
    "            classes=1,\n",
    "            activation='sigmoid'\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pad(x)\n",
    "        out =  self.model(x)\n",
    "        out = F.crop(out, 14, 14, 420, 580)\n",
    "        return out\n",
    "\n",
    "class LinkNet(nn.Module):\n",
    "    def __init__(self, imsize):\n",
    "            super(LinkNet, self).__init__()\n",
    "            self.imsize = imsize\n",
    "            self.pad = nn.ConstantPad2d(14, 0)\n",
    "            self.model = smp.Linknet(\n",
    "                encoder_name=\"resnet34\",\n",
    "                encoder_weights=\"imagenet\",\n",
    "                in_channels=1,\n",
    "                classes=1,\n",
    "                activation='sigmoid'\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pad(x)\n",
    "        out =  self.model(x)\n",
    "        out = F.crop(out, 14, 14, 420, 580)\n",
    "        return out\n",
    "\n",
    "class MAnet(nn.Module):\n",
    "    def __init__(self, imsize):\n",
    "        super(MAnet, self).__init__()\n",
    "        self.imsize = imsize\n",
    "        self.pad = nn.ConstantPad2d(14, 0)\n",
    "        self.model = smp.MAnet(\n",
    "            encoder_name=\"resnet34\",\n",
    "            encoder_weights=\"imagenet\",\n",
    "            in_channels=1,\n",
    "            classes=1,\n",
    "            activation='sigmoid'\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pad(x)\n",
    "        out =  self.model(x)\n",
    "        out = F.crop(out, 14, 14, 420, 580)\n",
    "        return out\n",
    "\n",
    "class DeepLabV3Plus(nn.Module):\n",
    "    def __init__(self, imsize):\n",
    "        super(DeepLabV3Plus, self).__init__()\n",
    "        self.imsize = imsize\n",
    "        self.pad = nn.ConstantPad2d(14, 0)\n",
    "        self.model = smp.DeepLabV3Plus(\n",
    "            encoder_name=\"resnet34\",\n",
    "            encoder_weights=\"imagenet\",\n",
    "            in_channels=1,\n",
    "            classes=1,\n",
    "            activation='sigmoid'\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pad(x)\n",
    "        out =  self.model(x)\n",
    "        out = F.crop(out, 14, 14, 420, 580)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MONAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def vit():\n",
    "#     model = nets.ViT(in_channels=1, img_size=420*580, patch_size=8)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def swin():\n",
    "#     return timm.create_model('swin_base_patch4_window7_448')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルを選択する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-14T08:35:27.214779Z",
     "iopub.status.busy": "2023-02-14T08:35:27.213947Z",
     "iopub.status.idle": "2023-02-14T08:35:54.709427Z",
     "shell.execute_reply": "2023-02-14T08:35:54.708244Z",
     "shell.execute_reply.started": "2023-02-14T08:35:27.214739Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def select_model(model_str: str):\n",
    "    model = None\n",
    "    model_str = model_str.lower()\n",
    "    if model_str == \"debug\":\n",
    "        model = ModelForDebug()\n",
    "    elif model_str == 'unet_bn':\n",
    "        model = UNetBatchNorm(imsize=420*580)\n",
    "    elif model_str == 'unet_plus_plus':\n",
    "        model = UNetPlusPlus(imsize=420*580)\n",
    "    elif model_str == 'manet':\n",
    "        model = MAnet(imsize=420*580)\n",
    "    elif model_str == 'pspnet':\n",
    "        model = PSPNet(imsize=420*580)\n",
    "    elif model_str == 'linknet':\n",
    "        model = LinkNet(imsize=420*580)\n",
    "    elif model_str == 'deep_lab_v3_plus':\n",
    "        model = DeepLabV3Plus(imsize=420*580)\n",
    "    # TODO ここから下は未実装\n",
    "    # elif model_str == 'vit':\n",
    "    #     model = Vit()\n",
    "    # elif model_str == 'swin':\n",
    "    #     model = swin()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 損失関数と指標定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-14T08:35:18.101572Z",
     "iopub.status.busy": "2023-02-14T08:35:18.101097Z",
     "iopub.status.idle": "2023-02-14T08:35:18.121334Z",
     "shell.execute_reply": "2023-02-14T08:35:18.120399Z",
     "shell.execute_reply.started": "2023-02-14T08:35:18.101530Z"
    }
   },
   "outputs": [],
   "source": [
    "class DiceLoss():\n",
    "    def __call__(self, prediction, target):\n",
    "        loss = self.calculate_loss(prediction, target)\n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_loss(prediction, target):\n",
    "        epsilon = 1\n",
    "\n",
    "        dice_prediction = prediction.sum(dim=[1, 2, 3])    # 要素iには、バッチiの、各ピクセルについて陽性である確率の総和\n",
    "        dice_label = target.sum(dim=[1, 2, 3])    # 要素iには、バッチiの陽性ラベルの数\n",
    "        dice_correct = (target * prediction).sum(dim=[1, 2, 3])\n",
    "        \n",
    "        dice_ratio = (2 * dice_correct + epsilon) / (dice_prediction + dice_label + epsilon)\n",
    "\n",
    "        return 1 - dice_ratio\n",
    "    \n",
    "    \n",
    "class SegmentationMetrics():\n",
    "\n",
    "    def __call__(self, prediction, target):\n",
    "        metrics_dict = self.calculate_metrics(prediction, target)\n",
    "        return metrics_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_metrics(prediction, target):\n",
    "        epsilon = 1e-7\n",
    "        prediction = (prediction > 0.5).to(torch.float32)\n",
    "        dice_prediction = prediction.sum(dim=[1, 2, 3])\n",
    "        dice_label = target.sum(dim=[1, 2, 3])\n",
    "        dice_correct = (prediction * target).sum(dim=[1, 2, 3])\n",
    "      \n",
    "        dice_coefficient = (2 * dice_correct + epsilon) / (dice_prediction + dice_label + epsilon)\n",
    "  \n",
    "        precision = (dice_correct + epsilon) / (dice_prediction + epsilon)\n",
    "        recall = (dice_correct + epsilon) / (dice_label + epsilon)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        \n",
    "        metrics_dict = {\n",
    "            'dice_coefficient': dice_coefficient.mean(),\n",
    "            'precision': precision.mean(),\n",
    "            'recall': recall.mean(),\n",
    "            'f1': f1.mean()\n",
    "        }\n",
    "\n",
    "        return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Lightning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitSegmentationModule(pl.LightningModule):\n",
    "    def __init__(self, model, lr=0.01):\n",
    "        super().__init__()\n",
    "        self.model = select_model(model_str=model)\n",
    "        self.lr = lr\n",
    "        self.save_hyperparameters()\n",
    "        self.example_input_array = torch.Tensor(8, 1, 420, 580)\n",
    "        self.loss_func = self.configure_loss_function()\n",
    "        self.metrics_calculater = self.configure_metrics()\n",
    "        self.training_step_outputs = {}\n",
    "        self.validation_step_outputs = {}\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        img, target = batch['img'], batch['target']\n",
    "        target = target.view(img.shape)\n",
    "        prediction = self.model(img).view(img.shape)\n",
    "        loss = self.loss_func(prediction, target)\n",
    "        loss_mean = loss.mean()\n",
    "        self.log('train_loss', loss_mean, on_epoch=True)\n",
    "        output_dict = self.draw_mask_on_image(batch, prediction, target)\n",
    "        output_dict['loss'] = loss\n",
    "        self.training_step_outputs = output_dict\n",
    "\n",
    "        return loss_mean\n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        img, target = batch['img'], batch['target']\n",
    "        target = target.view(img.shape)\n",
    "        prediction = self.model(img)\n",
    "        loss = self.loss_func(prediction, target)\n",
    "        loss_mean = loss.mean()\n",
    "        metrics = self.metrics_calculater(prediction, target)\n",
    "        metrics['val_loss'] = loss_mean\n",
    "        self.log_dict(metrics, on_epoch=True)\n",
    "        output_dict = self.draw_mask_on_image(batch, prediction, target)\n",
    "        output_dict['loss'] = loss\n",
    "        self.validation_step_outputs = output_dict\n",
    "        return loss_mean\n",
    "\n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        if not self.training_step_outputs:\n",
    "            return\n",
    "        training_outputs = self.training_step_outputs\n",
    "        validation_outputs = self.validation_step_outputs\n",
    "        logger = self.logger.experiment\n",
    "        \n",
    "        epochs = self.current_epoch\n",
    "        \n",
    "        for mode_str, output in zip(['training', 'val'], [training_outputs, validation_outputs]):\n",
    "            self.add_image(mode_str, output, epochs, logger)\n",
    "        \n",
    "    def draw_mask_on_image(self, batch, predictions, target):\n",
    "        self.model.eval()\n",
    "\n",
    "        # visualize image\n",
    "        imgs_vis = batch['img_vis'].cpu()\n",
    "\n",
    "        # target mask\n",
    "        targets = torch.squeeze(target, 1)\n",
    "        targets = targets.to(torch.bool).cpu()\n",
    "\n",
    "        # output mask\n",
    "        predictions = predictions >= 0.5\n",
    "        predictions = torch.squeeze(predictions, 1).cpu()\n",
    "\n",
    "        # input image\n",
    "        rg = torch.zeros(imgs_vis.shape[0:1] +  (2,) + imgs_vis.shape[2:4], dtype=torch.uint8).cpu()\n",
    "        imgs_rgb = torch.cat((rg, imgs_vis), 1).to(dtype=torch.uint8)\n",
    "\n",
    "        # mask\n",
    "        output_with_mask = [draw_segmentation_masks(img_rgb, masks=output, alpha=.3, colors=\"#FFFFFF\") for img_rgb, output in zip(imgs_rgb, predictions)]\n",
    "        target_with_mask = [draw_segmentation_masks(img_rgb, masks=target, alpha=.3, colors=\"#FFFFFF\") for img_rgb, target in zip(imgs_rgb, targets)]\n",
    "        \n",
    "        output_dict = {'output': output_with_mask, 'target': target_with_mask}\n",
    "        return output_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def add_image(mode_str, output, epochs, logger):\n",
    "        for image_ndx, (output_mask, target_mask, loss) in enumerate(zip(output['output'], output['target'], output['loss'])):\n",
    "\n",
    "            logger.add_image(f'{mode_str}/E{epochs}_#{image_ndx}_prediction_{loss.item():.3f}loss', output_mask, dataformats='CHW')\n",
    "            logger.add_image(f'{mode_str}/E{epochs}_#{image_ndx}_label', target_mask, dataformats='CHW')\n",
    "            logger.flush()\n",
    "\n",
    "    \n",
    "    def configure_loss_function(self):\n",
    "#         loss_func = torchmetrics.Dice(zero_division=1e-7, num_classes=2, average=None)\n",
    "        loss_func = DiceLoss()\n",
    "        return loss_func\n",
    "\n",
    "    def configure_metrics(self):\n",
    "        metrics_calculater = SegmentationMetrics()\n",
    "        return metrics_calculater\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-07 00:25:14,704 - Global seed set to 42\n",
      "2023-03-07 00:25:15,108 - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "2023-03-07 00:25:15,123 - GPU available: True (cuda), used: True\n",
      "2023-03-07 00:25:15,124 - TPU available: False, using: 0 TPU cores\n",
      "2023-03-07 00:25:15,124 - IPU available: False, using: 0 IPUs\n",
      "2023-03-07 00:25:15,125 - HPU available: False, using: 0 HPUs\n",
      "2023-03-07 00:25:15,127 - Missing logger folder: /home/ec2-user/SageMaker/working/logs/20230307_09.25_DeepLabV3Plus_balance_logs\n",
      "<__main__.UltrasoundNerveDataset object at 0x7f5741e36f70>: 3756 training samples\n",
      "<__main__.UltrasoundNerveDataset object at 0x7f5741e365e0>: 1879 validation samples\n",
      "<__main__.UltrasoundNerveDataset object at 0x7f5741e365b0>: 5635 training samples\n",
      "2023-03-07 00:25:15,314 - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "2023-03-07 00:25:16,061 - `Trainer.fit` stopped: `max_steps=3` reached.\n",
      "2023-03-07 00:25:16,062 - Batch size 2 succeeded, trying batch size 4\n",
      "2023-03-07 00:25:16,907 - `Trainer.fit` stopped: `max_steps=3` reached.\n",
      "2023-03-07 00:25:16,908 - Batch size 4 succeeded, trying batch size 8\n",
      "2023-03-07 00:25:18,413 - `Trainer.fit` stopped: `max_steps=3` reached.\n",
      "2023-03-07 00:25:18,414 - Batch size 8 succeeded, trying batch size 16\n",
      "2023-03-07 00:25:21,120 - `Trainer.fit` stopped: `max_steps=3` reached.\n",
      "2023-03-07 00:25:21,121 - Batch size 16 succeeded, trying batch size 32\n",
      "2023-03-07 00:25:26,379 - `Trainer.fit` stopped: `max_steps=3` reached.\n",
      "2023-03-07 00:25:26,379 - Batch size 32 succeeded, trying batch size 64\n",
      "2023-03-07 00:25:28,048 - Batch size 64 failed, trying batch size 32\n",
      "2023-03-07 00:25:28,258 - Finished batch size finder, will continue with full run using batch size 32\n",
      "2023-03-07 00:25:28,259 - Restoring states from the checkpoint path at /home/ec2-user/SageMaker/working/.scale_batch_size_fccbb46f-f4f0-48f8-be61-2d735ce6f774.ckpt\n",
      "2023-03-07 00:25:28,385 - Restored all states from the checkpoint file at /home/ec2-user/SageMaker/working/.scale_batch_size_fccbb46f-f4f0-48f8-be61-2d735ce6f774.ckpt\n",
      "<__main__.UltrasoundNerveDataset object at 0x7f573240b280>: 3756 training samples\n",
      "<__main__.UltrasoundNerveDataset object at 0x7f5741e36f70>: 1879 validation samples\n",
      "<__main__.UltrasoundNerveDataset object at 0x7f5741e365e0>: 5635 training samples\n",
      "2023-03-07 00:25:28,674 - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011249780654907227,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Finding best initial lr",
       "rate": null,
       "total": 100,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "616e20aa4a614a448a711f029d72d621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-07 00:28:32,246 - `Trainer.fit` stopped: `max_steps=100` reached.\n",
      "2023-03-07 00:28:32,248 - Learning rate set to 0.0009120108393559097\n",
      "2023-03-07 00:28:32,249 - Restoring states from the checkpoint path at /home/ec2-user/SageMaker/working/.lr_find_61a8b2a1-501b-4c9e-856e-44e79a140070.ckpt\n",
      "2023-03-07 00:28:32,464 - Restored all states from the checkpoint file at /home/ec2-user/SageMaker/working/.lr_find_61a8b2a1-501b-4c9e-856e-44e79a140070.ckpt\n",
      "<__main__.UltrasoundNerveDataset object at 0x7f57321908e0>: 3756 training samples\n",
      "<__main__.UltrasoundNerveDataset object at 0x7f573869b3a0>: 1879 validation samples\n",
      "<__main__.UltrasoundNerveDataset object at 0x7f583cb3b130>: 5635 training samples\n",
      "2023-03-07 00:28:32,743 - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "2023-03-07 00:28:32,773 - \n",
      "   | Name                                       | Type                 | Params | In sizes                                                                                                       | Out sizes                                                                                                     \n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "0  | model                                      | DeepLabV3Plus        | 22.4 M | [8, 1, 420, 580]                                                                                               | [8, 1, 420, 580]                                                                                              \n",
      "1  | model.pad                                  | ConstantPad2d        | 0      | [8, 1, 420, 580]                                                                                               | [8, 1, 448, 608]                                                                                              \n",
      "2  | model.model                                | DeepLabV3Plus        | 22.4 M | [8, 1, 448, 608]                                                                                               | [8, 1, 448, 608]                                                                                              \n",
      "3  | model.model.encoder                        | ResNetEncoder        | 21.3 M | [8, 1, 448, 608]                                                                                               | [[8, 1, 448, 608], [8, 64, 224, 304], [8, 64, 112, 152], [8, 128, 56, 76], [8, 256, 28, 38], [8, 512, 28, 38]]\n",
      "4  | model.model.encoder.conv1                  | Conv2d               | 3.1 K  | [8, 1, 448, 608]                                                                                               | [8, 64, 224, 304]                                                                                             \n",
      "5  | model.model.encoder.bn1                    | BatchNorm2d          | 128    | [8, 64, 224, 304]                                                                                              | [8, 64, 224, 304]                                                                                             \n",
      "6  | model.model.encoder.relu                   | ReLU                 | 0      | [8, 64, 224, 304]                                                                                              | [8, 64, 224, 304]                                                                                             \n",
      "7  | model.model.encoder.maxpool                | MaxPool2d            | 0      | [8, 64, 224, 304]                                                                                              | [8, 64, 112, 152]                                                                                             \n",
      "8  | model.model.encoder.layer1                 | Sequential           | 221 K  | [8, 64, 112, 152]                                                                                              | [8, 64, 112, 152]                                                                                             \n",
      "9  | model.model.encoder.layer1.0               | BasicBlock           | 74.0 K | [8, 64, 112, 152]                                                                                              | [8, 64, 112, 152]                                                                                             \n",
      "10 | model.model.encoder.layer1.1               | BasicBlock           | 74.0 K | [8, 64, 112, 152]                                                                                              | [8, 64, 112, 152]                                                                                             \n",
      "11 | model.model.encoder.layer1.2               | BasicBlock           | 74.0 K | [8, 64, 112, 152]                                                                                              | [8, 64, 112, 152]                                                                                             \n",
      "12 | model.model.encoder.layer2                 | Sequential           | 1.1 M  | [8, 64, 112, 152]                                                                                              | [8, 128, 56, 76]                                                                                              \n",
      "13 | model.model.encoder.layer2.0               | BasicBlock           | 230 K  | [8, 64, 112, 152]                                                                                              | [8, 128, 56, 76]                                                                                              \n",
      "14 | model.model.encoder.layer2.1               | BasicBlock           | 295 K  | [8, 128, 56, 76]                                                                                               | [8, 128, 56, 76]                                                                                              \n",
      "15 | model.model.encoder.layer2.2               | BasicBlock           | 295 K  | [8, 128, 56, 76]                                                                                               | [8, 128, 56, 76]                                                                                              \n",
      "16 | model.model.encoder.layer2.3               | BasicBlock           | 295 K  | [8, 128, 56, 76]                                                                                               | [8, 128, 56, 76]                                                                                              \n",
      "17 | model.model.encoder.layer3                 | Sequential           | 6.8 M  | [8, 128, 56, 76]                                                                                               | [8, 256, 28, 38]                                                                                              \n",
      "18 | model.model.encoder.layer3.0               | BasicBlock           | 919 K  | [8, 128, 56, 76]                                                                                               | [8, 256, 28, 38]                                                                                              \n",
      "19 | model.model.encoder.layer3.1               | BasicBlock           | 1.2 M  | [8, 256, 28, 38]                                                                                               | [8, 256, 28, 38]                                                                                              \n",
      "20 | model.model.encoder.layer3.2               | BasicBlock           | 1.2 M  | [8, 256, 28, 38]                                                                                               | [8, 256, 28, 38]                                                                                              \n",
      "21 | model.model.encoder.layer3.3               | BasicBlock           | 1.2 M  | [8, 256, 28, 38]                                                                                               | [8, 256, 28, 38]                                                                                              \n",
      "22 | model.model.encoder.layer3.4               | BasicBlock           | 1.2 M  | [8, 256, 28, 38]                                                                                               | [8, 256, 28, 38]                                                                                              \n",
      "23 | model.model.encoder.layer3.5               | BasicBlock           | 1.2 M  | [8, 256, 28, 38]                                                                                               | [8, 256, 28, 38]                                                                                              \n",
      "24 | model.model.encoder.layer4                 | Sequential           | 13.1 M | [8, 256, 28, 38]                                                                                               | [8, 512, 28, 38]                                                                                              \n",
      "25 | model.model.encoder.layer4.0               | BasicBlock           | 3.7 M  | [8, 256, 28, 38]                                                                                               | [8, 512, 28, 38]                                                                                              \n",
      "26 | model.model.encoder.layer4.1               | BasicBlock           | 4.7 M  | [8, 512, 28, 38]                                                                                               | [8, 512, 28, 38]                                                                                              \n",
      "27 | model.model.encoder.layer4.2               | BasicBlock           | 4.7 M  | [8, 512, 28, 38]                                                                                               | [8, 512, 28, 38]                                                                                              \n",
      "28 | model.model.decoder                        | DeepLabV3PlusDecoder | 1.2 M  | [[8, 1, 448, 608], [8, 64, 224, 304], [8, 64, 112, 152], [8, 128, 56, 76], [8, 256, 28, 38], [8, 512, 28, 38]] | [8, 256, 112, 152]                                                                                            \n",
      "29 | model.model.decoder.aspp                   | Sequential           | 1.1 M  | [8, 512, 28, 38]                                                                                               | [8, 256, 28, 38]                                                                                              \n",
      "30 | model.model.decoder.aspp.0                 | ASPP                 | 999 K  | [8, 512, 28, 38]                                                                                               | [8, 256, 28, 38]                                                                                              \n",
      "31 | model.model.decoder.aspp.1                 | SeparableConv2d      | 67.8 K | [8, 256, 28, 38]                                                                                               | [8, 256, 28, 38]                                                                                              \n",
      "32 | model.model.decoder.aspp.2                 | BatchNorm2d          | 512    | [8, 256, 28, 38]                                                                                               | [8, 256, 28, 38]                                                                                              \n",
      "33 | model.model.decoder.aspp.3                 | ReLU                 | 0      | [8, 256, 28, 38]                                                                                               | [8, 256, 28, 38]                                                                                              \n",
      "34 | model.model.decoder.up                     | UpsamplingBilinear2d | 0      | [8, 256, 28, 38]                                                                                               | [8, 256, 112, 152]                                                                                            \n",
      "35 | model.model.decoder.block1                 | Sequential           | 3.2 K  | [8, 64, 112, 152]                                                                                              | [8, 48, 112, 152]                                                                                             \n",
      "36 | model.model.decoder.block1.0               | Conv2d               | 3.1 K  | [8, 64, 112, 152]                                                                                              | [8, 48, 112, 152]                                                                                             \n",
      "37 | model.model.decoder.block1.1               | BatchNorm2d          | 96     | [8, 48, 112, 152]                                                                                              | [8, 48, 112, 152]                                                                                             \n",
      "38 | model.model.decoder.block1.2               | ReLU                 | 0      | [8, 48, 112, 152]                                                                                              | [8, 48, 112, 152]                                                                                             \n",
      "39 | model.model.decoder.block2                 | Sequential           | 81.1 K | [8, 304, 112, 152]                                                                                             | [8, 256, 112, 152]                                                                                            \n",
      "40 | model.model.decoder.block2.0               | SeparableConv2d      | 80.6 K | [8, 304, 112, 152]                                                                                             | [8, 256, 112, 152]                                                                                            \n",
      "41 | model.model.decoder.block2.1               | BatchNorm2d          | 512    | [8, 256, 112, 152]                                                                                             | [8, 256, 112, 152]                                                                                            \n",
      "42 | model.model.decoder.block2.2               | ReLU                 | 0      | [8, 256, 112, 152]                                                                                             | [8, 256, 112, 152]                                                                                            \n",
      "43 | model.model.segmentation_head              | SegmentationHead     | 257    | [8, 256, 112, 152]                                                                                             | [8, 1, 448, 608]                                                                                              \n",
      "44 | model.model.segmentation_head.0            | Conv2d               | 257    | [8, 256, 112, 152]                                                                                             | [8, 1, 112, 152]                                                                                              \n",
      "45 | model.model.segmentation_head.1            | UpsamplingBilinear2d | 0      | [8, 1, 112, 152]                                                                                               | [8, 1, 448, 608]                                                                                              \n",
      "46 | model.model.segmentation_head.2            | Activation           | 0      | [8, 1, 448, 608]                                                                                               | [8, 1, 448, 608]                                                                                              \n",
      "47 | model.model.segmentation_head.2.activation | Sigmoid              | 0      | [8, 1, 448, 608]                                                                                               | [8, 1, 448, 608]                                                                                              \n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "22.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "22.4 M    Total params\n",
      "89.725    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007863759994506836,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3a0689abf1140e09ae3ab619076d021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007091522216796875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-07 00:39:36,137 - Metric val_loss improved. New best score: 0.418\n",
      "2023-03-07 00:39:36,140 - Epoch 2, global step 354: 'val_loss' reached 0.41771 (best 0.41771), saving model to '/home/ec2-user/SageMaker/working/logs/20230307_09.25_DeepLabV3Plus_balance_logs/version_0/checkpoints/epoch=2-step=354.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006882429122924805,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-07 00:50:24,771 - Epoch 5, global step 708: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00739741325378418,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-07 01:01:58,544 - Epoch 8, global step 1062: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011005878448486328,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-07 01:13:29,920 - Monitored metric val_loss did not improve in the last 3 records. Best score: 0.418. Signaling Trainer to stop.\n",
      "2023-03-07 01:13:29,924 - Epoch 11, global step 1416: 'val_loss' was not in top 1\n"
     ]
    }
   ],
   "source": [
    "# root_dir\n",
    "default_root_dir = os.path.join(os.path.dirname(os.path.abspath(os.getcwd())), 'working')\n",
    "log_dir = os.path.join(default_root_dir, 'logs')\n",
    "\n",
    "# Reproducibility\n",
    "seed_everything = pl.seed_everything(42, workers=True)\n",
    "\n",
    "# Dataloader\n",
    "only_nerve_imgs = False\n",
    "balance = True\n",
    "data_module = UltrasoundDataModule()\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', verbose=True, patience=3)\n",
    "model_checkpoint = ModelCheckpoint(monitor='val_loss', verbose=True)\n",
    "model_summary = ModelSummary(max_depth=5)\n",
    "\n",
    "callbacks = [early_stopping, model_checkpoint, model_summary]\n",
    "\n",
    "# Model\n",
    "# model = \"debug\", \"Init\", \"UNet\", \"UNet_Pad\", \"UNet_BN\", \"UNet_with_library\", \"MANet\", \"Vit\", \"Swin\"\n",
    "model = LitSegmentationModule(model=\"Deep_Lab_v3_plus\")\n",
    "\n",
    "# Logger\n",
    "model_name = type(model.model).__name__\n",
    "data_mode = 'only_nerve' if only_nerve_imgs else 'balance' if balance else 'full_data'\n",
    "JST = datetime.timezone(datetime.timedelta(hours=+9), 'JST')\n",
    "now_str = datetime.datetime.now(JST).strftime('%Y%m%d_%H.%M')\n",
    "log_name = '_'.join([now_str, model_name, data_mode, 'logs'])\n",
    "tensorboard_logger = TensorBoardLogger(save_dir=log_dir, name=log_name)\n",
    "\n",
    "\n",
    "# Training\n",
    "\n",
    "# TODO When submit the final results, change benchmark=False, and deterministic=True to ensure reproducibility.\n",
    "trainer = pl.Trainer(fast_dev_run=False, devices='auto', accelerator='auto', default_root_dir=default_root_dir, logger=tensorboard_logger,  callbacks=callbacks, max_epochs=40, check_val_every_n_epoch=3, auto_scale_batch_size='power', auto_lr_find=True, benchmark=False, num_sanity_val_steps=0, enable_progress_bar=True)\n",
    "trainer.tune(model, data_module)\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timm.list_models('*swin*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
